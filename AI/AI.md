# 1. 지도학습
사람이 데이터를 어떻게 판단하거나 처리하는지에 대한 정답을 인공지능에게 알려주면서 진행하는 방식.

# 2. 인공지능
## 2-1. 강 인공지능 (Strong AI, General AI)
인간의 명령 없이 스스로 판단하고 결정을 내리기도 함.

다양한 상황에 유연하게 대처하며, 사람들과 자연스럽게 대화할 수 있다. 

매우 범용성을 가지는 인공지능, 범 인공지능이라고도 함.


## 2-2. 약 인공지능 (Weak AI, Narrow AI)
일상생활에서 만나는 모든 인공지능.

제한된 환경에서 구체적인 특정 업무를 수행하는 데 있어서 사람과 비슷한, 또는 사람 이상의 성능을 낼 수 있는 인공지능.

예) 구글 딥마인드의 알파고, IBM의 암 진단 인공지능

AI > Machine Learning > Deep Learning

- 딥러닝 : 머신러닝이라고 불리는 기계학습의 한 분야로, 기계가 비정형 데이터를 입력받은 후 데이터의 주요한 특징을 알아서 추출하고 이를 바탕으로 의사결정을 하는 기술.

### 머신러닝이 다루는 정형 데이터 (structured data)
흔히 데이터라는 말을 들었을 때 떠오르는 형태.

사람이 정제하고 정리한 데이터.

안정성은 높으나 체계적으로 구조가 고정되어 있어 유연하지 않은 데이터 타입.

머신 러닝은 이러한 타입의 데이터를 다루는 데 특화되어 있다.

실수 값 예측에 쓰이는 선형/로지스틱 회귀분석, 카테고리 분류에 쓰이는 위사 결정 나무, 시계열 예측에 쓰이는 ARMA, ARIMA 모형이 대표적.

### 딥러닝이 다루는 비정형 데이터 (unstructured data)
사람이 양식을 정리해놓지 않은, 다양한 형식을 가지는 데이터.


## 모라벡의 역설
사람에게 쉬운 것이 기계에게는 어렵다. 기계에게 쉬운 것은 사람에게 어렵다.


# 3. 인공 신경망 (Artificial Neuron)

# 4. CNN (Convolutional Neural Network)
이미지 처리에 특화된 신경망.

이미지로부터 특징을 추출하는 Feature Extraction 영역과, 특정 테스트 수행을 위해 덧붙이는 태스크 수행 영역 두 가지로 구성되어 있다.

## 4-1. 특징 추출 (Feature Extraction)

이미지로부터 특징을 추출하는 역할은 컨볼루션(Convolution) 연산과 풀링(Pooling)연산이 수행한다.

- 컨볼루션 연산: 컨볼루션 필터(또는 커널)가 입력 이미지를 상하좌우로 훑으며 주요한 특징이 있는지 찾아내는 과정. 이렇게 찾아낸 결과 특징을 Feature map(convolved feature)이라고 부른다. 특징을 찾는 작업은 인공 뉴런이 하는 일과 마찬가지로, 가중합 + 비선형 함수 적용으로 구성된다.

- 풀링 연산: Feature map을 상하좌우로 훑으며 핵심적인 정보만을 영역별로 샘플링한다. 주로 영역 내 가장 큰 값만을 남기고 나머지 값을 버리는 MaxPooling 방식을 적용한다.

대부분의 이미지 처리 모델에서는 컨볼루션과 풀링 연산을 여러 번 반복하면서 데이터의 feature을 추려낸다.

이미지로부터 특징을 배워나가는 작업이라는 뜻에서 이 과정을 Feature Learning이라 부른다.

## 4-2. 태스트 수행

찾아낸 주요 특징으로 해당 정보를 활용하여 목표로 하는 태스크를 수행해야 한다.

대표적은 태스크

### 1) Classification
입력으로 받은 이미지를 지정된 K개의 클래스(또는 카테고리) 중 하나로 분류하는 과제.

### 2) Detection
입력으로 받은 이미지에서 특정 개체가 어디에 위치하는지 좌표값을 찾아주는 과제.

### 3) Segmentation
Detection 보다 조금 더 정밀하게, 픽셀 단위로 영역을 구별해준다.


# 5. 자연어 이해 (NLU, Natural Language Understanding)

## Tokenizing(Parsing)

한 덩이로 되어 있는 문장을 인공 신경망에 인식시키기 위해서는 세부 단위로 쪼개는 작업이 필요하다. 이를 토크나이징/파싱 이라고 하며, 쪼개진 단위를 토큰이라고 부른다.

## 워드 임베딩 (Word embedding)

쪼개진 토큰들은 인공신경망이 계산할 수 있도록 벡터로 바꿔줘야 한다. 토큰을 벡터화하는 것을 워드임베딩이라고 한다.

토큰을 벡터화하는 워드임베딩 기법

### 1) 원-핫 인코딩 (One-hot Encoding)

가장 쉽고, 직관적인 방법으로, 모든 토큰을 줄세워 사전을 만들고, 순서대로 번호를 붙이는 작업.

문제점

- 토큰이 다양하고 수가 많을수록 토큰 하나를 표현하기 위해 굉장히 길이가 긴 벡터를 필요로 한다.

### 2) CBOW와 SKIPGRAM

먼저 토큰을 특정 길이를 가진 임의 벡터로 만들어준다. (원-핫 인코딩 방식보다 훨씬 작은 길이)

CBOW 방식은 인공지능에게 문장을 알려주되, 중간중간 빈칸을 만들어 들어갈 토큰을 유추시킨다.

SKIPGRAM 방식은 인공지능에게 토큰 하나를 알려주고, 주변에 등장할만한 그럴싸한 문맥을 만들도록 시킨다.

=> 많은 문장을 학습시키면 시킬 수록 더 좋은 품질의 벡터가 나온다.

## 다양한 자연어 이해 과제들
- 문장/문서 분류 (Sentence/Document Classification)

- Sequence-to-Sequece: 문장 또는 문서를 입력받아 문장을 출력하는 과제

- 질의 응답 (Question Answering)

# 6. 시간 흐름에 따른 데이터(Sequential data) 처리하기

## 6-1. Recurrent Neural Network(RNN, 순환 신경망)

기존의 신경망이 벡터, 또는 매트릭스로 변환된 입력 데이터를 가지고 출력을 한다면 RNN 역시 마찬가지 작업을 하지만 하나 다른 점이 있다. 과거에 데이터를 처리하여 결과를 출력했던 과정의 일부를 가져와 현 시점에서 데이터를 처리하고 결과를 출력하는 데 도움을 준다. 입력 데이터만으로 예측하는 것보다 과거의 누적된 정보가 있다면 더 개선된 예측이 가능할 것.

### 장점

- RNN 은 시간 흐름에 따른 과거 정보를 누적할 수 있다.
    - 입력 데이터 뿐 아니라 과거의 처리 내역을 반영하여 더 나은 결정을 할 수 있다.
- RNN은 가변 길이의 데이터를 처리할 수 있다.
    - 데이터 길이에 무관하게 자유롭게 구성할 수 있다.
- RNN은 다양한 구성의 모델을 만들 수 있다.
    - 유연한 구조를 가진 RNN은 다양한 구조를 활용하여 신경망을 구성할 수 있다.
    - 상황에 따라 다양하개 입력 데이터를 처리, 누적하고 결과를 예측할 수 있다. 입력 데이터의 정보를 누적하는 부분을 인코딩(Encoding), 결과를 출력하는 부분을 디코딩(Decoding)이라고 표현한다.

### 단점

- 연산 속도가 느리다.
    - 현 시점의 데이터를 처리하려면 반드시 이전 시점의 데이터가 처리 완료되어야 한다. 따라서 병렬 학습이 어렵고, 연산 속도가 다소 느린 편이다. (대부분 딥러닝 모델을 이용해 데이터 학습 시 GPU 서버를 활용하는데, GPU칩은 병렬 연산에 굉장한 이점을 가진 장비이다. 하지만 RNN을 처리하는 경우 이러한 이점을 잘 활용할 수 없다는 한계가 있다.)
    - 하지만, 정형 데이터를 활용하는 경우 속도 저하를 크게 체감하지 못하는 경우가 대부분이다.
    - 연산 속도 저하는 텍스트 데이터를 다루는 경우에 주로 문제가 된다.
- 학습이 불안정하다.
    - 단순 RNN은 학습시키기 매우 어려운 딥러닝 알고리즘 중 하나.
    - 다루는 데이터의 timestep이 길면 길수록 문제가 발생할 확률이 높다.
    - timestep이 길어지면 반영해야 할 과거 이력이 많아지게 된다. 이 과정에서 인공신경망이 학습해야할 값이 폭발적으로 증가하는 현상이 발생할 수 있다. 이를 Gradient Exploding이라 한다.
    - 이와 반대로, timestep이 길어지면 저 멀리 있는 과거의 이력은 현재의 추론에 거의 영향을 미치지 못하는 문제도 생긴다. 이를 Gradient Vanishing이라 한다.
    - RNN은 상당히 학습이 불안정해, 이 두 가지 문제가 자주 발생하곤 한다.
- 실질적으로 과거 정보를 잘 활용할 수 있는 모델이 아니다.
    - 먼 과거의 정보는 여러 번 압축되고 누적되다보니 거의 영향을 미치지 못한다. 이를 RNN의 장기 종속성/의존성 문제(Long-term dependency)라고 한다.

### 성능 보완

- LSTM (Long-short term memory, 장/단기 메모리 유닛)
    - 먼 과거의 정보 중 중요한 것은 기억하고, 불필요한 것은 잊어버리도록 스스로 조절 가능한 RNN 유닛이 있다.
    - 정보 흐름을 잘 조절하기 위해 성능을 개선한 특별한 형태의 뉴런.
    - 3 부분의 gate가 있다.
        - forget gate: 잊어버림에 대한 조절
        - input gate: 현재의 정보(input data)를 얼마나 반영할 지 결정
        - output gate: 현재 시점에 연산된 최종 정보를 다음 시점에 얼마나 넘길 지 결정

    - 계산 과정이 복잡한 만큼 연산 속도는 조금 더 느려지는 단점이 있다.
    - 이를 개선한 GRU(Gated Recurrent Unit)이 있다. (LSTM과 거의 유사한 기능)

- 오늘날 인공 신경망을 활용하는 대부분의 시계열 예측은 아주 간단한 과제를 제외하고는 기본 RNN을 사용하는 경우는 거의 없다. 대부분 LSTM, GRU와 같은 개선된 유닛을 활용.

# 7. AI process

## 7-1. Offline process (개발 과정)

- 과거에 만들어진 데이터를 가공하는 것부터 시작

- AI 모델은 확보된 데이터를 확인하고 정제해 필요한 부분을 취하거나 (Generate features) 필요한 경우 라벨을 붙인다. (Collect labels)

    - 라벨: AI 모델 학습을 위해 필요한 정보, 인공지능이 맞춰야하는 정답

- 데이터 마련 후, 어떤 머신러닝/딥러닝 알고리즘을 활용해 모델을 학습할 것인지 정하고, 좋은 성능을 달성할 때까지 (Validate&SElect models) 반복 실험을 진행한다. (Train models)

    - 대부분의 머신러닝/딥러닝 모델 개발의 경우 한 번의 시도만드로 최적 모델이 완성되는 경우는 거의 없다.

    - 보통, 실험의 결과를 진단하고 이 떄 발견한 문제점을 해결하거나 더 개선된 성능을 낼 수 있도록 실험(Experimentation)을 반복하는데, 이 과정을 "튜닝"이라 한다.

        - 튜닝: 모델 학습에 필요한 여러 수치 설정 값 (하이퍼 파라미터)을 조절하는 등의 역할을 포함.
    
- 여러 실험을 반복하며 개선된 성능의 모델은 배포할 수 있도록 최종 선택(Publish model) 된다.

=> 이러한 과정은 더 나은 AI모델을 만들기 위해 모델을 학습시키는 과정으로, Training Pipeline이라 한다.

## 7-2. Online process (운영 과정)

- 추론 (inference) 과정

- 모델 추론을 포함한 온라인 프로세스 대부분은 머신러닝/딥러닝 이라기보다는 개발 영역에 가깝다.

- AI 모델을 운영 환경에 띄움 (Load model)

- 해당 과정에서 모델이 처리할 데이터는 기존에 정리된 데이터가 아님. 실시간으로 들어오는 운영환경의 스트리밍 데이터(Live Data).

# 8. Generalization (일반화 성능)

이전에 본 적없는 데이터에 대해서도 잘 수행하는 능력.

- 훈련 시에만 잘 작동하고 일반화 성능이 떨어지는 모델을 오버피팅(Overfitting)되었다고 한다.

## 8-1. 데이터 셋

Training, Validation, Test
= 8:1:1 또는 6:2:2

### Training set

- 머신러닝/딥러닝 모델을 학습하는 데 이용하는 데이터.

- 모델은 해당 데이터 셋의 입력 데이터와 정답을 보고, 정답을 더 잘 맞추기 위해 노력함. 이는 단순히 최적화(Optimization)에 해당됨.

### Validation set

- 모델에게 정답을 알려 줄 데이터는 아니지만, 우리가 모델을 튜닝하는 데 도움을 주는 데이터.

- 즉, 모델의 일반화 성능을 판단하여 이어질 실험을 계획하는 데 이용.

- 모델이 배우지 않았던 데이터(처음 보는 데이터)에 대해 얼마나 잘 맞추는 지를 계산할 수 있어 이 결과를 보고 실험을 개선하게 된다.

### Test set

- 모델의 학습에 어떤 식으로도 전혀 관여하지 않는 데이터. 오로지 모델의 최종 성능을 평가하기 위해 따로 떼어놓은 데이터.

- 여러 모델간 성능 비교시 Test set에 대한 스코어를 활용.


## 학습 곡선 (Learning curve) 확인

- 학습 곡선: 학습이 진행됨에 따라 모델의 성능을 기록하는 그래프.

- 오버 피팅이 발생했는 지 확인할 수 있는 방법

    - 학습 곡선 상에서 Training set과 Validation set에 대한 모델의 성능이 어떻게 변화하는 지 확인

    - 어느 순간 Validation set에 대해서 성능 개선이 없다면 모델은 학습이 아닌 암기를 하는 것.

## Regularization

오버피팅을 피하기 위한 모든 전략을 말한다.

- 목적: 일반화 성능 향상

이를 위한 여러가지 방법

### 1) 데이터 증강 (Data Augmentation)

- 오버피팅을 피하는 가장 좋은 방법은 데이터를 더 많이 확보하는 것.

- 이미지 데이터 예) 좌우 반전, 일부 영역 크롭, 노이즈 추가, 색상/명암/채도 변화. 과도한 변형을 오히려 해가 될 수 있다.

### 2) Capacity 줄이기

- Capacity: 모델의 복잡한 정도

- 일반적으로 딥러닝 모델이 머신러닝 모델보다 높다. 그 중에서도 신경망을 여러 층 쌓거나 뉴런의 수를 많이 둘 수록 높아질 수 있다.

- Capacity가 높은 모델은 처리할 데이터의 복잡 다양한 패턴을 더 잘 담아낼 수 있다. 하지만 필요 이상으로 너무 높다면 주어진 데이터를 외우게 될 가능성이 높다. 따라서 수행하려는 태스트에 맞는 Capacity 모델을 선택하는 것이 좋다. 만일 오버피팅의 경향이 발견된다면, 모델의 층 수를 줄이거나, 한 층의 뉴런 수를 줄인다든지의 조치를 취할 필요가 있다.

### 3) 조기 종료 (Early stopping)

- 오버피팅이 감지될 경우 목표하는 학습 시간이 다 되지 않아도 조기종료해버리는 것.

- 적용하기 쉽고, 불필요하게 학습되는 경우를 방지함

### 4) 드롭 아웃(Dropout)

- 학습 과정(Training pipeline)에서 일정 비율만큼 노드(인공 뉴런)을 무작위로 끄고 진행하는 기법.

- 딥러닝 모델 학습 시 굉장히 많이 적용하는 기법 중 하나.

# 9. Transfer Learning (전이학습)

한 번 만든 인공지능 모델 우려먹기!

비슷한 태스크를 다른 도메인에 적용할 때, 그 태스크를 위한 학습 데이터가 부족한 경우 유용하게 쓰일 수 있다.

- 보통 오픈도메인 데이터에 대해 만들어 놓은 모델을 특정 도메인의 태스크에 적용하는 식으로 활용한다. (일반적인 내용을 두루두루 넓게 학습해놓은 모델의 사전 지식을 구체적인 하위 영역에 활용하는 셈)

## Catastrophic forgetting

태스크에 맞게 수정하여 추가로 학습하면 모든 것이 좋아질 듯 하지만, 만능 해결책은 아님.

딥러닝 모델이 새로운 정보를 학습할 때 이전에 배웠던 정보를 완전히 잊어 버리는 경향이 발생할 수 있다. 이를 Catastrophic forgetting이라 한다.

## 더 나은 Transfer Learning을 위한 방법

- 이미 학습해두었던 모델은 해당 데이터의 기본적인 특징을 어느정도 알고 있다. 따라서 새 태스크에 기 보유 모델을 Transfer Learning하려할 땐 기본 특징 학습은 건너 뛰고 바로 태스크를 위한 학습으로 가는 것이 좋다. 이를 위해 후반부의 신경망 층에 대해서만 파라미터를 학습하고, 전반부의 파라미터는 학습되지 않도록 고정해놓는 기법을 적용할 수 있다. 이를 레이어 동결(Layer freezing)이라 한다.

- 또는 다 동결시켜놓았다가 학습이 진행될 수록 후반부부터 서서히 동결을 푸는 방법(Gradual Unfreezing)도 적용 가능 하다.

- 층마다 Leaning rate(학습률)의 차별을 두는 것도 한 방법(Discriminative fine-tuning)

    - Learning rate: 한 번에 인공신경망의 파라미터를 얼마만큼 업데이트 시킬 지에 대한 정도. 사람이 설정하는 값(하이퍼파라미터)이다.

# 10. Pre-training (사전학습)

여러 태스크에 활용하기 위해 여러 지식을 미리 두루두루 학습해놓은 인공 지능.

이러한 학습을 Pre-training 또는 사전학습이라고 하며, 이러한 용도의 모델을 Pre-trained Model 즉 사전 학습 모델이라 한다.

## Self-Supervised Leanrinig (자가지도학습)

사람이 만들어주는 정답 라벨이 없어도 기계가 시스템적으로 자체 라벨을 만들어서 사용하는 학습 방법.

- 주로 사전학습에서 이용되며 다량의 데이터는 있으나 라벨은 없는 경우에 활용 가능하다.


# Active Learning (능동 학습)

데이터는 많으나 인공지능을 학습시킬 데이터를 마련하기 쉽지 않을 때 이용 가능한 기술

라벨링을 할 수 있는 인적 자원은 있지만, 많은 수의 라벨링을 수행할 수 없을 때 효과적으로 라벨링을 하기 위한 기법.

- 수행하고자 하는 태스크가 너무 특수해 해당 도메인의 전문 인력만이 데이터를 라벨링할 수 없는 경우, 최대한 학습에 효과적ㅇ인 데이터만 뽑아내는 데에 쓰일 수 있다.

- 학습 데이터 중 모델 성능 향상에 효과적인 데이터들을 선별한 후, 선별한 데이터를 활용해 학습을 진행하는 방법.

    - 학습 데이터를 확보하는 과정은 데이터를 수집하는 것과 수집한 데이터에 라벨을 태깅하는 라벨링ㅈ 작업으로 구성되어 있다. 일반적으로 라벨링 작업에 많은 시간과 인적 자원 활용 비용이 소요된다. 그렇기 때문에 같은 수의 데이터에 라벨을 붙여서 학습할 때 성능이 높게 나올 수 있도록 데이터를 선별한다면 효과적으로 딥러닝 모델을 학습할 수 있다. 이렇게 효과적인 데이터를 선별하는 방법을 연구하는 것이 Active Learning이다. 
    - 이와 반대로 주어진 라벨 데이터만 가지고 모델을 학습하는 방법을 Passive Learning(수동 학습)이라 한다.

## Active Learning의 절차

1. Traning a Model: 초기 학습 데이터(Labeled data)를 이용해 모델을 학습한다.
2. Select Query: 라벨이 되지 않은 데이터 풀로부터 모델에게 도움이 되는 데이터를 선별한다.
3. Human Labeling: 선별한 데이터를 사람이 확인하여 라벨을 태깅한다.
4. 선별한 라벨 데이터를 기존 학습 데이터와 병합한 후, 다시 모델을 학습한다.

목표하는 성능이 나올 때까지 위 방법 반복 수행.

## Query Strategy 

Active Learning의 핵심은 성능 향상에 효과적인 데이터를 선별하는 방법. 이러한 데이터 선별 방법을 쿼리 전략(Query Startegy)이라고 한다.

쿼리 전략을 어떻게 정하는냐에 따라서 선별할 데이터가 달라진다.

- 학습된 모델의 판정 값을 기반으로 뽑는 Uncertainty Sampling
- 여러 개의 모델을 동시에 학습시키면서 많은 모델이 틀리는 데이터를 선별하는 Query by committee
- 데이터가 학습 데이터로 추가될 떄, 학습된 모델이 가장 많이 변화하는 데이터를 선별하는 Expected Impact
- 데이터가 밀집된 지역의 데이터들을 선별하는 Density weighted method
- 데이터들을 최대한 고르게 뽑아서 전체 분포를 대표할 수 있도록 데이터를 선별하는 Core-set approach

### Uncertainty Sampling
- 가장 단순한 쿼리 전략
- AI 모델은 가장 불확실하다고 생각하는 데이터를 추출해 라벨링이 필요하다고 요청. 

### Query by committee
- 여러 AI모델 간의 의견 불일치를 종합 고려하는 방법.
- 여러 모델간 추론한 결과 불일치가 많은 데이터일수록 가장 헷갈리는 데이터, 즉 라벨링을 진행할 대상이 된다.

# 11. Attention Mechanism

RNN은 입력 문장의 단어 하나하나를 누적하여 압축해 인코딩하다가, 모든 문장이 다 들어오게 되면 영어로 한 단어씩 번역을 수행(디코딩)한다. 이 떄 디코더가 참고하는 문맥은 입력문이 전무 합축된 하나의 백터이다. 이 백터는 긴 문장을 모두 누적하고 있지만, 문장 앞 부분의 내용은 너무 압축된 나머지 정보를 거의 잊어버린 것이나 마찬가지.

여기에 어텐션 메커니즘을 끼얹어, 번역 시에 원문을 다시 재참조하며 현재 디코딩할 단어와 연관된 중요 부분에 집중하게 한다면?

해당 단어에 조금 더 집중하여 전체 입력을 다시 한번 재조정한 입력 데이터 인코딩 벡터를 만든다. 이렇게 하면 입력 문장이 매우 길어진다고 해도 전체 문맥을 골고루 참고할 수 있게 되므로 더 좋은 번역이 가능해진다.

## Attention Score

중요한 단어에 집중한다는 것은 어텐션 스코어를 계산한다는 것.

- 어텐션 스코어: 인공신경망 모델이 각 인코딩 timestep마다 계산된 특징(feature)를 가지고 자동으로 계산하는 0-1사이의 값

    - 어떤 step은 더 집중해서 봐야하고(1에 가까운 스코어), 어떤 스템은 지금은 중요하지 않으므로 대충 살피도록(0에 가까운 스코어) 하는 것.

## Context Vector

어텐션 스코어를 구하고 나면 현재 디콛징할 단어와의 관련성을 반영하여 다시 입력 문장을 인코딩하게 되는데, 이는 중요도에 따라 전체 문맥의 정보를 잘 반영하고 있다고 하여 컨텍스트 백터라고 부른다.

- 어텐션 메커니즘이 매번 디코딩마다 직전 단계의 벡터 뿐만 아니라 과거의 모든 데이터의 feature를 고려한다는 점이 중요!

- 추가로, 딥러닝 모델이 스스로 집중할 영역을 파악한다는 것!

## XAI로서의 어텐션

어텐션 메커니즘은 기계가 판단 시 중요하게 생각하는 부분을 우리에게 알려주는 역할도 한다. 이를 설명가능한 인공지능으로서의 기능을 수행하는 것이라고 함.

이러한 영역만을 따로 연구하는 분야가 있을 정도로 인공지능의 추론 결과를 해석하는 것은 중요한 영역. 이를 해석가능한 인공지능 이라고 부른다.

## Attention 전성시대, Transformer

- 트랜스포머라는 인공신경망: 입력 데이터끼리의 self-attention을 통해 상호 정보교환을 수행하는 것이 특징. 문장 내의 단어들이 서로 정보를 파악하며 나와 내 주변 단어간의 관계, 문맥을 더 잘 파악할 수 있게 되는 것.
    - 순차적 계산이 필요 없어 RNN보다 빠르면서도 맥락 파악을 잘하고, CNN처럼 일부만 보는 것이 아닌 전 영역을 아우른다. 
    - 이해력이 좋은 대신 모델의 크기가 엄청 커지며, 고사양의 하드웨어 스펙을 요구한다는 단점. 이러한 한계 보완을 위한 연구 진행 중







